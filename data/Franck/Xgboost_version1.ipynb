{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f64aa3b-eb4e-4353-bc7d-650bdc74f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 0.0_Dependance.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1797d83f-f16f-4e85-91c8-6e9d6906487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\",index_col=0)\n",
    "df[\"N_Year\"]=np.round((df['N_Days']/365),2)\n",
    "df.Age = np.round((df.Age)/365,1)\n",
    "\n",
    "#label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "colonne_a_encoder = [\"Sex\",\"Ascites\",\"Hepatomegaly\",\"Spiders\",\"Edema\",\"Drug\",\"Status\"]\t\n",
    "for i in colonne_a_encoder:\n",
    "    df[i] = le.fit_transform(df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "28629e0d-afd9-49d1-91b1-56253a8d1cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_initial = {\n",
    "    # Nombre d'arbres dans le boosting\n",
    "    'n_estimators': [50, 100, 250, 500, 1000],\n",
    "    \n",
    "    # Profondeur de chaque arbre\n",
    "    'max_depth': [3, 6, 9, 12],\n",
    "    \n",
    "    # Taux d'apprentissage (eta)\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    \n",
    "    # Minimum de perte de réduction nécessaire pour faire une partition supplémentaire sur un noeud de l'arbre\n",
    "    'gamma': [0, 0.1, 0.5, 1],\n",
    "    \n",
    "    # Fraction des observations à être randomisées pour chaque arbre\n",
    "    'subsample': [0.5, 0.75, 1],\n",
    "    \n",
    "    # Fraction de colonnes à être randomisées pour chaque arbre\n",
    "    'colsample_bytree': [0.5, 0.75, 1],\n",
    "    \n",
    "    # Le minimum de poids nécessaire pour une enfant\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    \n",
    "    # Regularisation L1 sur les poids\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    \n",
    "    # Regularisation L2 sur les poids\n",
    "    'reg_lambda': [0, 1, 10],\n",
    "    \n",
    "    # Choix de la fonction objectif, logloss pour la classification\n",
    "    'objective': ['multi:softprob'],\n",
    "    \n",
    "    # Choix de la stratégie d'évaluation pour le boosting\n",
    "    'eval_metric': ['mlogloss'],\n",
    "    \n",
    "    # Choix du booster\n",
    "    'booster': ['gbtree'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dda9f6c1-4a30-464d-859c-71f9f37685f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ameliore = {\n",
    "    # Nombre d'arbres dans le boosting\n",
    "    'n_estimators': [400,500,600,700,800,900],\n",
    "    \n",
    "    # Profondeur de chaque arbre\n",
    "    'max_depth': [4,5,6],\n",
    "    \n",
    "    # Taux d'apprentissage (eta)\n",
    "    'learning_rate': [0.05,0.06,0.07,0.08,0.09,0.1],\n",
    "    \n",
    "    # Minimum de perte de réduction nécessaire pour faire une partition supplémentaire sur un noeud de l'arbre\n",
    "    'gamma': [0.5,0.7, 1],\n",
    "    \n",
    "    # Fraction des observations à être randomisées pour chaque arbre\n",
    "    'subsample': [0.75, 1],\n",
    "    \n",
    "    # Fraction de colonnes à être randomisées pour chaque arbre\n",
    "    'colsample_bytree': [0.2,0.4,0.5,0.6],\n",
    "    \n",
    "    # Le minimum de poids nécessaire pour une enfant\n",
    "    'min_child_weight': [ 5,6,7,8,9,10],\n",
    "    \n",
    "    # Regularisation L1 sur les poids\n",
    "    'reg_alpha': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1],\n",
    "    \n",
    "    # Regularisation L2 sur les poids\n",
    "    'reg_lambda': [0, 1,2,3,4,5,6,7,8,9,10],\n",
    "    \n",
    "    # Choix de la fonction objectif, logloss pour la classification\n",
    "    'objective': ['multi:softprob'],\n",
    "    \n",
    "    # Choix de la stratégie d'évaluation pour le boosting\n",
    "    'eval_metric': ['mlogloss'],\n",
    "    \n",
    "    # Choix du booster\n",
    "    'booster': ['gbtree'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d6dbbdf0-bd57-4dce-9ef1-8de64d6e45d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récupération du DataFrames existant\n",
      "Début du randomized: RandomizedSearchCV(cv=5,\n",
      "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                           callbacks=None,\n",
      "                                           colsample_bylevel=None,\n",
      "                                           colsample_bynode=None,\n",
      "                                           colsample_bytree=None, device=None,\n",
      "                                           early_stopping_rounds=None,\n",
      "                                           enable_categorical=False,\n",
      "                                           eval_metric=None, feature_types=None,\n",
      "                                           gamma=None, grow_policy=None,\n",
      "                                           importance_type=None,\n",
      "                                           interaction_constraints=None,\n",
      "                                           learning_rate...\n",
      "                                        'eval_metric': ['mlogloss'],\n",
      "                                        'gamma': [0.5, 0.7, 1],\n",
      "                                        'learning_rate': [0.05, 0.06, 0.07,\n",
      "                                                          0.08, 0.09, 0.1],\n",
      "                                        'max_depth': [4, 5, 6],\n",
      "                                        'min_child_weight': [5, 6, 7, 8, 9, 10],\n",
      "                                        'n_estimators': [400, 500, 600, 700,\n",
      "                                                         800, 900],\n",
      "                                        'objective': ['multi:softprob'],\n",
      "                                        'reg_alpha': [0.1, 0.2, 0.3, 0.4, 0.5,\n",
      "                                                      0.6, 0.7, 0.8, 0.9, 1],\n",
      "                                        'reg_lambda': [0, 1, 2, 3, 4, 5, 6, 7,\n",
      "                                                       8, 9, 10],\n",
      "                                        'subsample': [0.75, 1]},\n",
      "                   scoring='neg_log_loss')\n",
      "Fin du randomized: RandomizedSearchCV(cv=5,\n",
      "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                           callbacks=None,\n",
      "                                           colsample_bylevel=None,\n",
      "                                           colsample_bynode=None,\n",
      "                                           colsample_bytree=None, device=None,\n",
      "                                           early_stopping_rounds=None,\n",
      "                                           enable_categorical=False,\n",
      "                                           eval_metric=None, feature_types=None,\n",
      "                                           gamma=None, grow_policy=None,\n",
      "                                           importance_type=None,\n",
      "                                           interaction_constraints=None,\n",
      "                                           learning_rate...\n",
      "                                        'eval_metric': ['mlogloss'],\n",
      "                                        'gamma': [0.5, 0.7, 1],\n",
      "                                        'learning_rate': [0.05, 0.06, 0.07,\n",
      "                                                          0.08, 0.09, 0.1],\n",
      "                                        'max_depth': [4, 5, 6],\n",
      "                                        'min_child_weight': [5, 6, 7, 8, 9, 10],\n",
      "                                        'n_estimators': [400, 500, 600, 700,\n",
      "                                                         800, 900],\n",
      "                                        'objective': ['multi:softprob'],\n",
      "                                        'reg_alpha': [0.1, 0.2, 0.3, 0.4, 0.5,\n",
      "                                                      0.6, 0.7, 0.8, 0.9, 1],\n",
      "                                        'reg_lambda': [0, 1, 2, 3, 4, 5, 6, 7,\n",
      "                                                       8, 9, 10],\n",
      "                                        'subsample': [0.75, 1]},\n",
      "                   scoring='neg_log_loss')\n",
      "146.6991093158722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['df_score_random_v1.joblib']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "joblib_path_suivi_metrique_1 = get_output_path_file(\"df_score_random_v1.joblib\")\n",
    "joblib_path_suivi_metrique_2 = get_output_path_file(\"df_score_best_test_v1.joblib\")\n",
    "\n",
    "target = df.Status\n",
    "data = df.drop(columns=[\"Status\",\"N_Days\"])\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(data,target,test_size=0.2, random_state=123)\n",
    "#Standardisation\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Modèle instancié\n",
    "model = XGBClassifier(n_estimators=1000, max_depth=2, learning_rate=0.1, objective=\"multi:softprob\")\n",
    "\n",
    "randomized_search_CV = RandomizedSearchCV(model, param_distributions=param_grid_ameliore,n_iter=100, cv=5, scoring='neg_log_loss',n_jobs=-1)\n",
    "df_score_random_v1 = test_randomized_model(joblib_path_suivi_metrique_1, param_grid_ameliore, model, randomized_search_CV)\n",
    "dump(df_score_random_v1,\"df_score_random_v1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1ac352b5-c9c1-4454-b01e-cf8fc574f4a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['df_score_random_v1.joblib']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(df_score_random_v1,\"df_score_random_v1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "729e43b8-4190-4099-84d5-d37f3278bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_random_v1 = load(\"df_score_random_v1.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
