{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f64aa3b-eb4e-4353-bc7d-650bdc74f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 0.0_Dependance.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1797d83f-f16f-4e85-91c8-6e9d6906487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\",index_col=0)\n",
    "df[\"N_Year\"]=np.round((df['N_Days']/365),2)\n",
    "df.Age = np.round((df.Age)/365,1)\n",
    "\n",
    "#label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "colonne_a_encoder = [\"Sex\",\"Ascites\",\"Hepatomegaly\",\"Spiders\",\"Edema\",\"Drug\",\"Status\"]\t\n",
    "for i in colonne_a_encoder:\n",
    "    df[i] = le.fit_transform(df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "28629e0d-afd9-49d1-91b1-56253a8d1cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_initial = {\n",
    "    # Nombre d'arbres dans le boosting\n",
    "    'n_estimators': [50, 100, 250, 500, 1000],\n",
    "    \n",
    "    # Profondeur de chaque arbre\n",
    "    'max_depth': [3, 6, 9, 12],\n",
    "    \n",
    "    # Taux d'apprentissage (eta)\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    \n",
    "    # Minimum de perte de réduction nécessaire pour faire une partition supplémentaire sur un noeud de l'arbre\n",
    "    'gamma': [0, 0.1, 0.5, 1],\n",
    "    \n",
    "    # Fraction des observations à être randomisées pour chaque arbre\n",
    "    'subsample': [0.5, 0.75, 1],\n",
    "    \n",
    "    # Fraction de colonnes à être randomisées pour chaque arbre\n",
    "    'colsample_bytree': [0.5, 0.75, 1],\n",
    "    \n",
    "    # Le minimum de poids nécessaire pour une enfant\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    \n",
    "    # Regularisation L1 sur les poids\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    \n",
    "    # Regularisation L2 sur les poids\n",
    "    'reg_lambda': [0, 1, 10],\n",
    "    \n",
    "    # Choix de la fonction objectif, logloss pour la classification\n",
    "    'objective': ['multi:softprob'],\n",
    "    \n",
    "    # Choix de la stratégie d'évaluation pour le boosting\n",
    "    'eval_metric': ['mlogloss'],\n",
    "    \n",
    "    # Choix du booster\n",
    "    'booster': ['gbtree'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dda9f6c1-4a30-464d-859c-71f9f37685f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ameliore = {\n",
    "    # Nombre d'arbres dans le boosting\n",
    "    'n_estimators': [400,500,600,700,800,900],\n",
    "    \n",
    "    # Profondeur de chaque arbre\n",
    "    'max_depth': [4,5,6],\n",
    "    \n",
    "    # Taux d'apprentissage (eta)\n",
    "    'learning_rate': [0.05,0.06,0.07,0.08,0.09,0.1],\n",
    "    \n",
    "    # Minimum de perte de réduction nécessaire pour faire une partition supplémentaire sur un noeud de l'arbre\n",
    "    'gamma': [0.5,0.7, 1],\n",
    "    \n",
    "    # Fraction des observations à être randomisées pour chaque arbre\n",
    "    'subsample': [0.75, 1],\n",
    "    \n",
    "    # Fraction de colonnes à être randomisées pour chaque arbre\n",
    "    'colsample_bytree': [0.2,0.4,0.5,0.6],\n",
    "    \n",
    "    # Le minimum de poids nécessaire pour une enfant\n",
    "    'min_child_weight': [ 5,6,7,8,9,10],\n",
    "    \n",
    "    # Regularisation L1 sur les poids\n",
    "    'reg_alpha': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1],\n",
    "    \n",
    "    # Regularisation L2 sur les poids\n",
    "    'reg_lambda': [0, 1,2,3,4,5,6,7,8,9,10],\n",
    "    \n",
    "    # Choix de la fonction objectif, logloss pour la classification\n",
    "    'objective': ['multi:softprob'],\n",
    "    \n",
    "    # Choix de la stratégie d'évaluation pour le boosting\n",
    "    'eval_metric': ['mlogloss'],\n",
    "    \n",
    "    # Choix du booster\n",
    "    'booster': ['gbtree'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d6dbbdf0-bd57-4dce-9ef1-8de64d6e45d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récupération du DataFrames existant\n",
      "Début du randomized: RandomizedSearchCV(cv=5,\n",
      "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                           callbacks=None,\n",
      "                                           colsample_bylevel=None,\n",
      "                                           colsample_bynode=None,\n",
      "                                           colsample_bytree=None, device=None,\n",
      "                                           early_stopping_rounds=None,\n",
      "                                           enable_categorical=False,\n",
      "                                           eval_metric=None, feature_types=None,\n",
      "                                           gamma=None, grow_policy=None,\n",
      "                                           importance_type=None,\n",
      "                                           interaction_constraints=None,\n",
      "                                           learning_rate...\n",
      "                                        'eval_metric': ['mlogloss'],\n",
      "                                        'gamma': [0.5, 0.7, 1],\n",
      "                                        'learning_rate': [0.05, 0.06, 0.07,\n",
      "                                                          0.08, 0.09, 0.1],\n",
      "                                        'max_depth': [4, 5, 6],\n",
      "                                        'min_child_weight': [5, 6, 7, 8, 9, 10],\n",
      "                                        'n_estimators': [400, 500, 600, 700,\n",
      "                                                         800, 900],\n",
      "                                        'objective': ['multi:softprob'],\n",
      "                                        'reg_alpha': [0.1, 0.2, 0.3, 0.4, 0.5,\n",
      "                                                      0.6, 0.7, 0.8, 0.9, 1],\n",
      "                                        'reg_lambda': [0, 1, 2, 3, 4, 5, 6, 7,\n",
      "                                                       8, 9, 10],\n",
      "                                        'subsample': [0.75, 1]},\n",
      "                   scoring='neg_log_loss')\n",
      "Fin du randomized: RandomizedSearchCV(cv=5,\n",
      "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                           callbacks=None,\n",
      "                                           colsample_bylevel=None,\n",
      "                                           colsample_bynode=None,\n",
      "                                           colsample_bytree=None, device=None,\n",
      "                                           early_stopping_rounds=None,\n",
      "                                           enable_categorical=False,\n",
      "                                           eval_metric=None, feature_types=None,\n",
      "                                           gamma=None, grow_policy=None,\n",
      "                                           importance_type=None,\n",
      "                                           interaction_constraints=None,\n",
      "                                           learning_rate...\n",
      "                                        'eval_metric': ['mlogloss'],\n",
      "                                        'gamma': [0.5, 0.7, 1],\n",
      "                                        'learning_rate': [0.05, 0.06, 0.07,\n",
      "                                                          0.08, 0.09, 0.1],\n",
      "                                        'max_depth': [4, 5, 6],\n",
      "                                        'min_child_weight': [5, 6, 7, 8, 9, 10],\n",
      "                                        'n_estimators': [400, 500, 600, 700,\n",
      "                                                         800, 900],\n",
      "                                        'objective': ['multi:softprob'],\n",
      "                                        'reg_alpha': [0.1, 0.2, 0.3, 0.4, 0.5,\n",
      "                                                      0.6, 0.7, 0.8, 0.9, 1],\n",
      "                                        'reg_lambda': [0, 1, 2, 3, 4, 5, 6, 7,\n",
      "                                                       8, 9, 10],\n",
      "                                        'subsample': [0.75, 1]},\n",
      "                   scoring='neg_log_loss')\n",
      "148.3471496105194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['df_score_random_v1.joblib']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "joblib_path_suivi_metrique_1 = get_output_path_file(\"df_score_random_v1.joblib\")\n",
    "joblib_path_suivi_metrique_2 = get_output_path_file(\"df_score_best_test_v1.joblib\")\n",
    "\n",
    "target = df.Status\n",
    "data = df.drop(columns=[\"Status\",\"N_Days\"])\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(data,target,test_size=0.2, random_state=123)\n",
    "#Standardisation\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Modèle instancié\n",
    "model = XGBClassifier(n_estimators=1000, max_depth=2, learning_rate=0.1, objective=\"multi:softprob\")\n",
    "\n",
    "randomized_search_CV = RandomizedSearchCV(model, param_distributions=param_grid_ameliore,n_iter=100, cv=5, scoring='neg_log_loss',n_jobs=-1)\n",
    "df_score_random_v1 = test_randomized_model(joblib_path_suivi_metrique_1, param_grid_ameliore, model, randomized_search_CV)\n",
    "dump(df_score_random_v1,\"df_score_random_v1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1ac352b5-c9c1-4454-b01e-cf8fc574f4a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['df_score_random_v1.joblib']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(df_score_random_v1,\"df_score_random_v1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "729e43b8-4190-4099-84d5-d37f3278bc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>gamma</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>objective</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>booster</th>\n",
       "      <th>LogLoss_mean</th>\n",
       "      <th>LogLoss_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.430215</td>\n",
       "      <td>0.014266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>900</td>\n",
       "      <td>4</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.436219</td>\n",
       "      <td>0.015202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>900</td>\n",
       "      <td>6</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.442235</td>\n",
       "      <td>0.013131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>800</td>\n",
       "      <td>6</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>9</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.433717</td>\n",
       "      <td>0.014389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>900</td>\n",
       "      <td>4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.434770</td>\n",
       "      <td>0.012334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>500</td>\n",
       "      <td>6</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.441835</td>\n",
       "      <td>0.014290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>600</td>\n",
       "      <td>6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.431759</td>\n",
       "      <td>0.011003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>600</td>\n",
       "      <td>5</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.429887</td>\n",
       "      <td>0.013815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>400</td>\n",
       "      <td>4</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>8</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.442534</td>\n",
       "      <td>0.014449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>900</td>\n",
       "      <td>4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.442891</td>\n",
       "      <td>0.015946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators max_depth learning_rate gamma subsample colsample_bytree  \\\n",
       "0            500         5          0.08   0.7      0.75              0.2   \n",
       "1            900         4          0.09   0.5      0.75              0.2   \n",
       "2            900         6          0.08     1         1              0.5   \n",
       "3            800         6          0.08   0.5      0.75              0.2   \n",
       "4            900         4          0.05   0.5         1              0.2   \n",
       "..           ...       ...           ...   ...       ...              ...   \n",
       "195          500         6          0.09     1      0.75              0.6   \n",
       "196          600         6          0.05     1         1              0.2   \n",
       "197          600         5          0.08   0.7      0.75              0.2   \n",
       "198          400         4          0.09   0.7         1              0.4   \n",
       "199          900         4          0.05   0.5      0.75              0.5   \n",
       "\n",
       "    min_child_weight reg_alpha reg_lambda       objective eval_metric booster  \\\n",
       "0                  9       0.3          2  multi:softprob    mlogloss  gbtree   \n",
       "1                 10       0.6          1  multi:softprob    mlogloss  gbtree   \n",
       "2                  5       0.3          5  multi:softprob    mlogloss  gbtree   \n",
       "3                  7       0.6          9  multi:softprob    mlogloss  gbtree   \n",
       "4                  6       0.9          3  multi:softprob    mlogloss  gbtree   \n",
       "..               ...       ...        ...             ...         ...     ...   \n",
       "195               10       0.9          0  multi:softprob    mlogloss  gbtree   \n",
       "196                6       0.3          0  multi:softprob    mlogloss  gbtree   \n",
       "197                7       0.6          4  multi:softprob    mlogloss  gbtree   \n",
       "198                7       0.2          8  multi:softprob    mlogloss  gbtree   \n",
       "199                6       0.6          3  multi:softprob    mlogloss  gbtree   \n",
       "\n",
       "     LogLoss_mean  LogLoss_std  \n",
       "0       -0.430215     0.014266  \n",
       "1       -0.436219     0.015202  \n",
       "2       -0.442235     0.013131  \n",
       "3       -0.433717     0.014389  \n",
       "4       -0.434770     0.012334  \n",
       "..            ...          ...  \n",
       "195     -0.441835     0.014290  \n",
       "196     -0.431759     0.011003  \n",
       "197     -0.429887     0.013815  \n",
       "198     -0.442534     0.014449  \n",
       "199     -0.442891     0.015946  \n",
       "\n",
       "[200 rows x 14 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score_random_v1 = load(\"df_score_random_v1.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
