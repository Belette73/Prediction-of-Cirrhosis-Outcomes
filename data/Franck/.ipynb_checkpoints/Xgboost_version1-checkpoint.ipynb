{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f64aa3b-eb4e-4353-bc7d-650bdc74f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 0.0_Dependance.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1797d83f-f16f-4e85-91c8-6e9d6906487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\",index_col=0)\n",
    "df[\"N_Year\"]=np.round((df['N_Days']/365),2)\n",
    "df.Age = np.round((df.Age)/365,1)\n",
    "\n",
    "#label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "colonne_a_encoder = [\"Sex\",\"Ascites\",\"Hepatomegaly\",\"Spiders\",\"Edema\",\"Drug\",\"Status\"]\t\n",
    "for i in colonne_a_encoder:\n",
    "    df[i] = le.fit_transform(df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28629e0d-afd9-49d1-91b1-56253a8d1cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_initial = {\n",
    "    # Nombre d'arbres dans le boosting\n",
    "    'n_estimators': [50, 100, 250, 500, 1000],\n",
    "    \n",
    "    # Profondeur de chaque arbre\n",
    "    'max_depth': [3, 6, 9, 12],\n",
    "    \n",
    "    # Taux d'apprentissage (eta)\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    \n",
    "    # Minimum de perte de réduction nécessaire pour faire une partition supplémentaire sur un noeud de l'arbre\n",
    "    'gamma': [0, 0.1, 0.5, 1],\n",
    "    \n",
    "    # Fraction des observations à être randomisées pour chaque arbre\n",
    "    'subsample': [0.5, 0.75, 1],\n",
    "    \n",
    "    # Fraction de colonnes à être randomisées pour chaque arbre\n",
    "    'colsample_bytree': [0.5, 0.75, 1],\n",
    "    \n",
    "    # Le minimum de poids nécessaire pour une enfant\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    \n",
    "    # Regularisation L1 sur les poids\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    \n",
    "    # Regularisation L2 sur les poids\n",
    "    'reg_lambda': [0, 1, 10],\n",
    "    \n",
    "    # Choix de la fonction objectif, logloss pour la classification\n",
    "    'objective': ['multi:softprob'],\n",
    "    \n",
    "    # Choix de la stratégie d'évaluation pour le boosting\n",
    "    'eval_metric': ['mlogloss'],\n",
    "    \n",
    "    # Choix du booster\n",
    "    'booster': ['gbtree'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dda9f6c1-4a30-464d-859c-71f9f37685f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ameliore = {\n",
    "    # Nombre d'arbres dans le boosting\n",
    "    'n_estimators': [400,500,600,700,800,900],\n",
    "    \n",
    "    # Profondeur de chaque arbre\n",
    "    'max_depth': [4,5,6],\n",
    "    \n",
    "    # Taux d'apprentissage (eta)\n",
    "    'learning_rate': [0.05,0.06,0.07,0.08,0.09,0.1],\n",
    "    \n",
    "    # Minimum de perte de réduction nécessaire pour faire une partition supplémentaire sur un noeud de l'arbre\n",
    "    'gamma': [0.5,0.7, 1],\n",
    "    \n",
    "    # Fraction des observations à être randomisées pour chaque arbre\n",
    "    'subsample': [0.75, 1],\n",
    "    \n",
    "    # Fraction de colonnes à être randomisées pour chaque arbre\n",
    "    'colsample_bytree': [0.2,0.4,0.5,0.6],\n",
    "    \n",
    "    # Le minimum de poids nécessaire pour une enfant\n",
    "    'min_child_weight': [ 5,6,7,8,9,10],\n",
    "    \n",
    "    # Regularisation L1 sur les poids\n",
    "    'reg_alpha': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1],\n",
    "    \n",
    "    # Regularisation L2 sur les poids\n",
    "    'reg_lambda': [0, 1,2,3,4,5,6,7,8,9,10],\n",
    "    \n",
    "    # Choix de la fonction objectif, logloss pour la classification\n",
    "    'objective': ['multi:softprob'],\n",
    "    \n",
    "    # Choix de la stratégie d'évaluation pour le boosting\n",
    "    'eval_metric': ['mlogloss'],\n",
    "    \n",
    "    # Choix du booster\n",
    "    'booster': ['gbtree'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f423920c-eb16-4698-90dc-e1f74e1a20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ameliore2 = {\n",
    "    # Nombre d'arbres dans le boosting\n",
    "    'n_estimators': [300,400,500,600],\n",
    "    \n",
    "    # Profondeur de chaque arbre\n",
    "    'max_depth': [4,5,6],\n",
    "    \n",
    "    # Taux d'apprentissage (eta)\n",
    "    'learning_rate': [0.05],\n",
    "    \n",
    "    # Minimum de perte de réduction nécessaire pour faire une partition supplémentaire sur un noeud de l'arbre\n",
    "    'gamma': [0.5,0.7, 1],\n",
    "    \n",
    "    # Fraction des observations à être randomisées pour chaque arbre\n",
    "    'subsample': [0.75, 1],\n",
    "    \n",
    "    # Fraction de colonnes à être randomisées pour chaque arbre\n",
    "    'colsample_bytree': [0.2],\n",
    "    \n",
    "    # Le minimum de poids nécessaire pour une enfant\n",
    "    'min_child_weight': [5,6,7],\n",
    "    \n",
    "    # Regularisation L1 sur les poids\n",
    "    'reg_alpha': [0.4,0.5,0.6],\n",
    "    \n",
    "    # Regularisation L2 sur les poids\n",
    "    'reg_lambda': [0, 1,2,3,4,5,6,7,8,9,10],\n",
    "    \n",
    "    # Choix de la fonction objectif, logloss pour la classification\n",
    "    'objective': ['multi:softprob'],\n",
    "    \n",
    "    # Choix de la stratégie d'évaluation pour le boosting\n",
    "    'eval_metric': ['mlogloss'],\n",
    "    \n",
    "    # Choix du booster\n",
    "    'booster': ['gbtree'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6dbbdf0-bd57-4dce-9ef1-8de64d6e45d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récupération du DataFrames existant\n",
      "Début du randomized: RandomizedSearchCV(cv=5,\n",
      "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                           callbacks=None,\n",
      "                                           colsample_bylevel=None,\n",
      "                                           colsample_bynode=None,\n",
      "                                           colsample_bytree=None, device=None,\n",
      "                                           early_stopping_rounds=None,\n",
      "                                           enable_categorical=False,\n",
      "                                           eval_metric=None, feature_types=None,\n",
      "                                           gamma=None, grow_policy=None,\n",
      "                                           importance_type=None,\n",
      "                                           interaction_constraints=None,\n",
      "                                           learning_rate...\n",
      "                   param_distributions={'booster': ['gbtree'],\n",
      "                                        'colsample_bytree': [0.2],\n",
      "                                        'eval_metric': ['mlogloss'],\n",
      "                                        'gamma': [0.5, 0.7, 1],\n",
      "                                        'learning_rate': [0.05],\n",
      "                                        'max_depth': [4, 5, 6],\n",
      "                                        'min_child_weight': [5, 6, 7],\n",
      "                                        'n_estimators': [300, 400, 500, 600],\n",
      "                                        'objective': ['multi:softprob'],\n",
      "                                        'reg_alpha': [0.4, 0.5, 0.6],\n",
      "                                        'reg_lambda': [0, 1, 2, 3, 4, 5, 6, 7,\n",
      "                                                       8, 9, 10],\n",
      "                                        'subsample': [0.75, 1]},\n",
      "                   scoring='neg_log_loss')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 7128 is smaller than n_iter=30000. Running 7128 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin du randomized: RandomizedSearchCV(cv=5,\n",
      "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                           callbacks=None,\n",
      "                                           colsample_bylevel=None,\n",
      "                                           colsample_bynode=None,\n",
      "                                           colsample_bytree=None, device=None,\n",
      "                                           early_stopping_rounds=None,\n",
      "                                           enable_categorical=False,\n",
      "                                           eval_metric=None, feature_types=None,\n",
      "                                           gamma=None, grow_policy=None,\n",
      "                                           importance_type=None,\n",
      "                                           interaction_constraints=None,\n",
      "                                           learning_rate...\n",
      "                   param_distributions={'booster': ['gbtree'],\n",
      "                                        'colsample_bytree': [0.2],\n",
      "                                        'eval_metric': ['mlogloss'],\n",
      "                                        'gamma': [0.5, 0.7, 1],\n",
      "                                        'learning_rate': [0.05],\n",
      "                                        'max_depth': [4, 5, 6],\n",
      "                                        'min_child_weight': [5, 6, 7],\n",
      "                                        'n_estimators': [300, 400, 500, 600],\n",
      "                                        'objective': ['multi:softprob'],\n",
      "                                        'reg_alpha': [0.4, 0.5, 0.6],\n",
      "                                        'reg_lambda': [0, 1, 2, 3, 4, 5, 6, 7,\n",
      "                                                       8, 9, 10],\n",
      "                                        'subsample': [0.75, 1]},\n",
      "                   scoring='neg_log_loss')\n",
      "6937.403536558151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['df_score_random_v1_1.joblib']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "joblib_path_suivi_metrique_1 = get_output_path_file(\"df_score_random_v1.joblib\")\n",
    "joblib_path_suivi_metrique_2 = get_output_path_file(\"df_score_best_test_v1.joblib\")\n",
    "\n",
    "target = df.Status\n",
    "data = df.drop(columns=[\"Status\",\"N_Days\"])\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(data,target,test_size=0.2, random_state=123)\n",
    "#Standardisation\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Modèle instancié\n",
    "model = XGBClassifier(n_estimators=1000, max_depth=2, learning_rate=0.1, objective=\"multi:softprob\")\n",
    "\n",
    "randomized_search_CV = RandomizedSearchCV(model, param_distributions=param_grid_ameliore2,n_iter=30000, cv=5, scoring='neg_log_loss',n_jobs=-1)\n",
    "df_score_random_v1_1 = test_randomized_model(joblib_path_suivi_metrique_1, param_grid_ameliore2, model, randomized_search_CV)\n",
    "dump(df_score_random_v1_1,\"df_score_random_v1_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5990ab3e-5dfd-47a2-bcea-09d367fb0c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['df_score_random_v1.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import load,dump\n",
    "dump(df_score_random_v1_1,\"df_score_random_v1_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "729e43b8-4190-4099-84d5-d37f3278bc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['df_score_random_v1_1.joblib']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import load,dump\n",
    "df_score_random_v1_1 = load(\"df_score_random_v1_1.joblib\")\n",
    "df_score_random_v1_1 = df_score_random_v1_1.drop_duplicates()\n",
    "dump(df_score_random_v1_1,\"df_score_random_v1_1.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57ddaec0-e960-424a-8cf4-bd7bba163c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>gamma</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>objective</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>booster</th>\n",
       "      <th>LogLoss_mean</th>\n",
       "      <th>LogLoss_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.430215</td>\n",
       "      <td>0.014266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>900</td>\n",
       "      <td>4</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.436219</td>\n",
       "      <td>0.015202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>900</td>\n",
       "      <td>6</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.442235</td>\n",
       "      <td>0.013131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>800</td>\n",
       "      <td>6</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>9</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.433717</td>\n",
       "      <td>0.014389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>900</td>\n",
       "      <td>4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.434770</td>\n",
       "      <td>0.012334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77984</th>\n",
       "      <td>600</td>\n",
       "      <td>6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>8</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.437358</td>\n",
       "      <td>0.012090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77985</th>\n",
       "      <td>600</td>\n",
       "      <td>6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>9</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.436323</td>\n",
       "      <td>0.012936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77986</th>\n",
       "      <td>600</td>\n",
       "      <td>6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>9</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.438068</td>\n",
       "      <td>0.012563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77987</th>\n",
       "      <td>600</td>\n",
       "      <td>6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>10</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.436671</td>\n",
       "      <td>0.012938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77988</th>\n",
       "      <td>600</td>\n",
       "      <td>6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>10</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.438656</td>\n",
       "      <td>0.012234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77756 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      n_estimators max_depth learning_rate gamma subsample colsample_bytree  \\\n",
       "0              500         5          0.08   0.7      0.75              0.2   \n",
       "1              900         4          0.09   0.5      0.75              0.2   \n",
       "2              900         6          0.08     1         1              0.5   \n",
       "3              800         6          0.08   0.5      0.75              0.2   \n",
       "4              900         4          0.05   0.5         1              0.2   \n",
       "...            ...       ...           ...   ...       ...              ...   \n",
       "77984          600         6          0.05     1         1              0.2   \n",
       "77985          600         6          0.05     1      0.75              0.2   \n",
       "77986          600         6          0.05     1         1              0.2   \n",
       "77987          600         6          0.05     1      0.75              0.2   \n",
       "77988          600         6          0.05     1         1              0.2   \n",
       "\n",
       "      min_child_weight reg_alpha reg_lambda       objective eval_metric  \\\n",
       "0                    9       0.3          2  multi:softprob    mlogloss   \n",
       "1                   10       0.6          1  multi:softprob    mlogloss   \n",
       "2                    5       0.3          5  multi:softprob    mlogloss   \n",
       "3                    7       0.6          9  multi:softprob    mlogloss   \n",
       "4                    6       0.9          3  multi:softprob    mlogloss   \n",
       "...                ...       ...        ...             ...         ...   \n",
       "77984                7       0.6          8  multi:softprob    mlogloss   \n",
       "77985                7       0.6          9  multi:softprob    mlogloss   \n",
       "77986                7       0.6          9  multi:softprob    mlogloss   \n",
       "77987                7       0.6         10  multi:softprob    mlogloss   \n",
       "77988                7       0.6         10  multi:softprob    mlogloss   \n",
       "\n",
       "      booster  LogLoss_mean  LogLoss_std  \n",
       "0      gbtree     -0.430215     0.014266  \n",
       "1      gbtree     -0.436219     0.015202  \n",
       "2      gbtree     -0.442235     0.013131  \n",
       "3      gbtree     -0.433717     0.014389  \n",
       "4      gbtree     -0.434770     0.012334  \n",
       "...       ...           ...          ...  \n",
       "77984  gbtree     -0.437358     0.012090  \n",
       "77985  gbtree     -0.436323     0.012936  \n",
       "77986  gbtree     -0.438068     0.012563  \n",
       "77987  gbtree     -0.436671     0.012938  \n",
       "77988  gbtree     -0.438656     0.012234  \n",
       "\n",
       "[77756 rows x 14 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score_random_v1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c372cd8-74c8-4479-8a6e-7de7ea35cd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['df_score_random_v1_top_1000.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from joblib import dump,load\n",
    "\n",
    "def load_create_dftop(df_path, n_top):\n",
    "    df = load(df_path)\n",
    "    df_sorted = df.sort_values(by=\"LogLoss_mean\", ascending=False)\n",
    "    df_top = df_sorted.iloc[:n_top, :]\n",
    "    return df_top\n",
    "\n",
    "# Fonction pour entraîner le modèle et calculer le log_loss\n",
    "def compute_log_loss(i, df, X_train, y_train, X_test, y_test):\n",
    "    dico = df.iloc[i, :-2].to_dict()\n",
    "    model = XGBClassifier(**dico)\n",
    "    model.fit(X_train, y_train)\n",
    "    probs = model.predict_proba(X_test)\n",
    "    return log_loss(y_test, probs)\n",
    "\n",
    "df_score_random_v1_top_1000 = load_create_dftop(\"df_score_random_v1.joblib\", 1000)\n",
    "\n",
    "# Parallélisation du calcul du log_loss\n",
    "log_losses = Parallel(n_jobs=-1)(delayed(compute_log_loss)(i, df_score_random_v1_top_1000, X_train, y_train, X_test, y_test) for i in range(1000))\n",
    "\n",
    "df_score_random_v1_top_1000['Test_log_loss'] = log_losses\n",
    "df_score_random_v1_top_1000 = df_score_random_v1_top_1000.sort_values(by=\"Test_log_loss\",ascending=True)\n",
    "dump(df_score_random_v1_top_1000,\"df_score_random_v1_top_1000.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "237686b1-6a4a-49bc-827e-1abb4a923404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>gamma</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>objective</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>booster</th>\n",
       "      <th>LogLoss_mean</th>\n",
       "      <th>LogLoss_std</th>\n",
       "      <th>Test_log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54802</th>\n",
       "      <td>400</td>\n",
       "      <td>6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.427643</td>\n",
       "      <td>0.012277</td>\n",
       "      <td>0.438500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69650</th>\n",
       "      <td>400</td>\n",
       "      <td>6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.429360</td>\n",
       "      <td>0.012945</td>\n",
       "      <td>0.438810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40952</th>\n",
       "      <td>600</td>\n",
       "      <td>6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.428350</td>\n",
       "      <td>0.012862</td>\n",
       "      <td>0.439039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34044</th>\n",
       "      <td>400</td>\n",
       "      <td>6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.427807</td>\n",
       "      <td>0.012810</td>\n",
       "      <td>0.439241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53891</th>\n",
       "      <td>400</td>\n",
       "      <td>6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.427414</td>\n",
       "      <td>0.012488</td>\n",
       "      <td>0.439437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17381</th>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.429741</td>\n",
       "      <td>0.014865</td>\n",
       "      <td>0.451678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38919</th>\n",
       "      <td>500</td>\n",
       "      <td>6</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.429689</td>\n",
       "      <td>0.014921</td>\n",
       "      <td>0.452398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10831</th>\n",
       "      <td>800</td>\n",
       "      <td>6</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.429614</td>\n",
       "      <td>0.014190</td>\n",
       "      <td>0.452574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48709</th>\n",
       "      <td>600</td>\n",
       "      <td>5</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.429740</td>\n",
       "      <td>0.014534</td>\n",
       "      <td>0.453049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62617</th>\n",
       "      <td>400</td>\n",
       "      <td>6</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>mlogloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>-0.429479</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.453457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      n_estimators max_depth learning_rate gamma subsample colsample_bytree  \\\n",
       "54802          400         6          0.05   0.5         1              0.2   \n",
       "69650          400         6          0.05   0.5      0.75              0.2   \n",
       "40952          600         6          0.05   0.5         1              0.2   \n",
       "34044          400         6          0.05   0.5         1              0.2   \n",
       "53891          400         6          0.05   0.5         1              0.2   \n",
       "...            ...       ...           ...   ...       ...              ...   \n",
       "17381          500         5           0.1   0.5      0.75              0.2   \n",
       "38919          500         6          0.09   0.5      0.75              0.2   \n",
       "10831          800         6          0.08   0.7      0.75              0.2   \n",
       "48709          600         5          0.09   0.7      0.75              0.2   \n",
       "62617          400         6          0.09   0.5      0.75              0.2   \n",
       "\n",
       "      min_child_weight reg_alpha reg_lambda       objective eval_metric  \\\n",
       "54802                6       0.3          1  multi:softprob    mlogloss   \n",
       "69650               10       0.2          1  multi:softprob    mlogloss   \n",
       "40952                5       0.2          6  multi:softprob    mlogloss   \n",
       "34044                5       0.5          1  multi:softprob    mlogloss   \n",
       "53891                6       0.1          2  multi:softprob    mlogloss   \n",
       "...                ...       ...        ...             ...         ...   \n",
       "17381                5       0.9          1  multi:softprob    mlogloss   \n",
       "38919                5       0.3          3  multi:softprob    mlogloss   \n",
       "10831                6       0.1          0  multi:softprob    mlogloss   \n",
       "48709                6       0.2          0  multi:softprob    mlogloss   \n",
       "62617                7       0.3          0  multi:softprob    mlogloss   \n",
       "\n",
       "      booster  LogLoss_mean  LogLoss_std  Test_log_loss  \n",
       "54802  gbtree     -0.427643     0.012277       0.438500  \n",
       "69650  gbtree     -0.429360     0.012945       0.438810  \n",
       "40952  gbtree     -0.428350     0.012862       0.439039  \n",
       "34044  gbtree     -0.427807     0.012810       0.439241  \n",
       "53891  gbtree     -0.427414     0.012488       0.439437  \n",
       "...       ...           ...          ...            ...  \n",
       "17381  gbtree     -0.429741     0.014865       0.451678  \n",
       "38919  gbtree     -0.429689     0.014921       0.452398  \n",
       "10831  gbtree     -0.429614     0.014190       0.452574  \n",
       "48709  gbtree     -0.429740     0.014534       0.453049  \n",
       "62617  gbtree     -0.429479     0.014706       0.453457  \n",
       "\n",
       "[1000 rows x 15 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score_random_v1_top_1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
